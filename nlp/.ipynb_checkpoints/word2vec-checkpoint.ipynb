{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = [\"the quick brown fox jumps over the lazy dogs\",\"yoyoyo you go home now to sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'],\n",
       " ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences= [s.split() for s in raw_sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型参数设置\n",
    "- size参数主要是用来设置神经网络的层数，Word2Vec 中的默认值是设置为100层\n",
    "- 较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置min_count参数进行控制\n",
    "- workers参数用于设置并发训练时候的线程数\n",
    "- iter (int, optional) – 迭代次数，默认为5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanbo6/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.068531215"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('dogs','you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanbo6/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dogs', 0.19583024084568024)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['fox'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanbo6/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.8279381e-03, -4.5803948e-03, -4.3366407e-03, -4.2034006e-03,\n",
       "       -4.7761789e-03,  1.4629976e-04,  1.2712990e-03, -1.2642483e-03,\n",
       "        4.7198229e-04,  2.1912607e-03,  3.6955557e-03,  3.9278282e-04,\n",
       "        2.0635526e-03, -2.3397161e-03, -3.5204191e-03, -1.1090651e-03,\n",
       "       -3.6213303e-03,  4.6683783e-03,  1.8415543e-03,  1.1086961e-03,\n",
       "       -3.3181333e-03, -4.3470273e-03,  1.8832835e-03, -3.8389515e-03,\n",
       "        2.9632221e-03,  4.3070698e-03,  4.5213709e-03,  3.3779645e-03,\n",
       "        2.8111890e-03, -1.2498076e-05,  2.0743930e-03, -6.6931348e-04,\n",
       "        9.1947394e-04, -1.8246139e-03, -2.2512733e-03,  3.8041412e-03,\n",
       "       -3.9251219e-03, -3.2807745e-03, -3.7123351e-03, -5.5184640e-04,\n",
       "       -2.0280594e-03, -1.5185041e-03,  8.1849808e-04,  8.4703771e-04,\n",
       "        1.7248170e-03, -1.5616586e-04, -4.7776797e-03, -3.7469242e-03,\n",
       "       -1.4706479e-03,  4.9885516e-03,  1.7364019e-04, -6.0741894e-04,\n",
       "       -1.3600697e-03, -5.9735859e-05, -3.9317305e-03, -1.3204930e-04,\n",
       "        8.1753713e-04,  4.2205737e-03, -1.7781188e-03, -2.2071938e-03,\n",
       "       -1.7179998e-03,  1.9085675e-03,  2.9931413e-03,  5.1139072e-05,\n",
       "        1.7738038e-03, -4.7393851e-03, -3.6163847e-03, -6.0648593e-04,\n",
       "        3.2756384e-03, -1.8931471e-03, -5.3141318e-04,  2.9855040e-03,\n",
       "        1.2627065e-03, -3.0295432e-03,  3.2462676e-03,  1.1830053e-03,\n",
       "       -4.7939774e-03, -1.8537657e-03, -2.4402158e-03,  1.9074655e-03,\n",
       "       -2.4432866e-04,  4.3066666e-03, -2.7250168e-03,  4.1919639e-03,\n",
       "       -2.8284783e-03, -3.8649642e-03, -3.6591162e-05, -3.2627529e-03,\n",
       "        1.8752263e-03, -8.1138680e-04, -2.3900033e-03,  4.9809301e-03,\n",
       "       -1.5817786e-03,  4.0344726e-03,  1.1998726e-03,  4.5453571e-03,\n",
       "       -3.2450499e-03,  1.4221601e-03,  2.6590489e-03,  1.3705397e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到某个单词的embedding向量\n",
    "model['dogs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型save和load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_word2vec.model')\n",
    "new_model = gensim.models.Word2Vec.load('mymodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存得到的词向量\n",
    "model.wv.save_word2vec_format(outp2, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LineSentence\n",
    "文件的每行是一个句子\n",
    "\n",
    "按行读取，不占用过大内存\n",
    "\n",
    "max_sentence_length是返回的每句话中元素的最大个数，limit=3是说读取sample_text.txt中的前三行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "infilename = 'sample_text.txt'\n",
    "lines = gensim.models.word2vec.LineSentence(infilename, max_sentence_length=10, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用skip-gram\n",
    "\n",
    "skip-gram 对低频词汇表现更好\n",
    "\n",
    "“Skip-gram: works well with small amount of the training data, represents well even rare words or phrases\n",
    "\n",
    "CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认的设置是CBOW，如你设置word2vec的模型参数sg=1\n",
    "\n",
    "gensim.models.word2vec.Word2Vec(sentences, sg=1)\n",
    "那么就是用的skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### window\n",
    "\n",
    "window是指两端到中心词的距离，你上面的例子window=1\n",
    "\n",
    "window=2，就是x, x, center, x, x\n",
    "\n",
    "window=4，就是x, x, x, x, center, x, x, x, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "new_model = Word2Vec.load(outp_modle)\n",
    "\n",
    "# 新的数据需要更新到词表中，在加载模型之后添加\n",
    "model.build_vocab(LineSentence(new_file),update=True)\n",
    "\n",
    "# 训练新模型\n",
    "trainedWordCount = model.train(LineSentence(inp_file), total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "# 打印增量\n",
    "print('update model, update words num is: ' + str(trainedWordCount))\n",
    "\n",
    "# 存储新模型\n",
    "model.save(outp_model)\n",
    "model.wv.save_word2vec_format(outp_vector, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算两个向量的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate embedding simliarity\n",
    "def simliarity(vector1, vector2):\n",
    "    return np.dot(vector1, vector2.T)/(np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
